{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fin-tuning LLM on a simple task using single GPU with fast inference\n",
    "\n",
    "_Authored by: [Mohammadreza Esmaeiliyan](https://github.com/MrzEsma)_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a59bf2a9e5015030"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, the attempt has been made to fine-tune an LLM in the simplest manner without adding unnecessary complexity, with a parameter count suitable for a Customer-level GPU, and then to perform inference using one of the fastest open-source inference engines, Vllm. I have tried to explain all the concepts and techniques used as far as possible; however, since there are many concepts and techniques to explain, I firstly gave a priority based on importance so that the more important concepts and techniques can be studied first. Secondly, since others have written these explanations well and in more detail in blogs, I have referred to these links. As the Iranian saying goes, \"In a house of wisdom, a few words suffice\" :)\n",
    "Let's get started.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "755fc90c27f1cb99"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a35eafbe37e4ad2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `peft` library, or parameter efficient fine tuning, has been created to fine-tune LLMs more efficiently. If we were to open and fine-tune the upper layers of the network traditionally like all neural networks, it would require a lot of processing and also a significant amount of VRAM. With the methods developed in recent papers, this library has been implemented for efficient fine-tuning of LLMs. Read more about peft here: [Hugging Face PEFT](https://huggingface.co/blog/peft). Importance level: 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64671b5d61ba9d57"
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the emergence of LLMs, a task known as Alignment was created, in which we try to produce outputs from LLMs that are compatible with our preferences. We start with simple supervised fine tuning or SFT in the first stage, and in the second stage, a mechanism for receiving feedback from users is created, and with other techniques, we make the LLM more aligned with our preferences. The `trl` library has been created for such a task, and this library is used in the first stage, which is SFT. For further reading on the Alignment task, see [OpenAI Blog on Instruction Following](https://openai.com/research/instruction-following#fn1) and [Hugging Face Blog on RLHF](https://huggingface.co/blog/rlhf). Importance level: 1\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1e7c704058c2373"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "261a8f52fe09202e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# General parameters\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"  # The model that you want to train from the Hugging Face hub\n",
    "dataset_name = \"yahma/alpaca-cleaned\"  # The instruction dataset to use\n",
    "new_model = \"llama-2-7b-alpaca\"  # The name for fine-tuned LoRA Adaptor\n",
    "\n",
    "# LoRA parameters\n",
    "lora_r = 64\n",
    "lora_alpha = lora_r * 2\n",
    "lora_dropout = 0.1\n",
    "target_modules = [\"q_proj\", \"v_proj\", 'k_proj']\n",
    "\n",
    "# QLoRA parameters\n",
    "load_in_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "bnb_4bit_use_double_quant = False\n",
    "\n",
    "# TrainingArguments parameters\n",
    "num_train_epochs = 1\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 1\n",
    "gradient_checkpointing = True\n",
    "learning_rate = 0.00015\n",
    "weight_decay = 0.01\n",
    "optim = \"paged_adamw_32bit\"\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_steps = -1\n",
    "warmup_ratio = 0.03\n",
    "group_by_length = True\n",
    "save_steps = 0\n",
    "logging_steps = 25\n",
    "\n",
    "# SFT parameters\n",
    "max_seq_length = None\n",
    "packing = False\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# Dataset parameters\n",
    "use_special_template = True\n",
    "response_template = ' ### Answer:'\n",
    "instruction_prompt_template = '\"### Human:\"'\n",
    "use_llama_like_model = True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96fccf9f7364bac6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "234ef91c9c1c0789"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "percent_of_train_dataset = 0.95\n",
    "other_columns = [i for i in dataset.column_names if i not in ['instruction', 'output', 'text']]\n",
    "dataset = dataset.remove_columns(other_columns)\n",
    "split_dataset = dataset.train_test_split(train_size=int(dataset.num_rows * percent_of_train_dataset), seed=19, shuffle=False)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "print(f\"Size of the train set: {len(train_dataset)}. Size of the validation set: {len(eval_dataset)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cc58fe0c4b229e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Two techniques, LoRA and QLoRA, are among the most important techniques of PEFT. In brief, LoRA aims to open only these layers for fine-tuning by constructing and adding a low-rank matrix to each of the model layers, thus neither changing the model weights nor requiring lengthy training, and the created weights are lightweight and can be produced multiple times, allowing multiple tasks to be fine-tuned with an LLM loaded into RAM. In the QLoRA technique, the weights are quantized to 4 bits, further reducing RAM consumption. Read about LoRA [here at Lightning AI](https://lightning.ai/pages/community/tutorial/lora-llm/) and about QLoRA [here at Hugging Face](https://huggingface.co/blog/4bit-transformers-bitsandbytes). For other efficient training methods, see [Hugging Face Docs on Performance Training](https://huggingface.co/docs/transformers/perf_train_gpu_one) and [SFT Trainer Enhancement](https://huggingface.co/docs/trl/main/en/sft_trainer#enhance-models-performances-using-neftune). Importance level: 2\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "382296d37668763c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32d8aa11a6d47e0d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a5216910d0a339a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bacbbc9ddd19504d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this blog, you can read about the best practices for fine-tuning LLMs [Sebastian Raschka's Magazine](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms?r=1h0eu9&utm_campaign=post&utm_medium=web). Importance level: 1\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56219e83015a7357"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a82c50bc69c3632b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
    "if not tokenizer.chat_template:\n",
    "    tokenizer.chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c86b66f59bee28dc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def special_formatting_prompts(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        text = f\"{instruction_prompt_template}{example['instruction'][i]}{example['input'][i]}\\n{response_template} {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "def normal_formatting_prompts(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        chat_temp = [{\"role\": \"system\", \"content\": example['instruction'][i]},\n",
    "                     {\"role\": \"user\", \"content\": {example['input'][i]}},\n",
    "                     {\"role\": \"assistant\", \"content\": example['output'][i]}]\n",
    "        text = tokenizer.apply_chat_template(chat_temp, tokenize=False)\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d3f935e03db79b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Regarding the chat template, let me briefly explain that to understand the structure of the conversation between the user and the model during model training, a series of reserved phrases are created to separate the user's message and the model's response, so the model precisely understands where each message comes from and has a sense of the conversational structure. Typically, adhering to a chat template helps increase accuracy in the intended task. However, when there is a distribution shift between the fine-tuning dataset and the model, using a specific chat template can be more helpful. For further reading, visit [Hugging Face Blog on Chat Templates](https://huggingface.co/blog/chat-templates). Importance level: 3\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea4399c36bcdcbbd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if use_special_template:\n",
    "    formatting_func = special_formatting_prompts\n",
    "    if use_llama_like_model:\n",
    "        response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)[2:]\n",
    "        collator = DataCollatorForCompletionOnlyLM(response_template=response_template_ids, tokenizer=tokenizer)\n",
    "    else:\n",
    "        collator = DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer)\n",
    "else:\n",
    "    formatting_func = normal_formatting_prompts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95dc3db0d6c5ddaf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=formatting_func,\n",
    "    data_collator=collator,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48e09edab86c4212"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save fine tuned Lora Adaptor \n",
    "trainer.model.save_pretrained(new_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a17a3b28010ce90e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference Code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39abd4f63776cc49"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "def clear_hardwares():\n",
    "    torch.clear_autocast_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70cca01bc96d9ead"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "clear_hardwares()\n",
    "clear_hardwares()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76760bc5f6c5c632"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate(model, prompt: str, kwargs):\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    prompt_length = len(tokenized_prompt.get('input_ids')[0])\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = model.generate(**tokenized_prompt, **kwargs) if kwargs else model.generate(**tokenized_prompt)\n",
    "        output = tokenizer.decode(output_tokens[0][prompt_length:], skip_special_tokens=True)\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd8313238b26e95e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(new_model, return_dict=True, device_map='auto', token='')\n",
    "tokenizer = AutoTokenizer.from_pretrained(new_model, max_length=max_seq_length)\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "del base_model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3fe5a27fa40ba9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample = eval_dataset[0]\n",
    "if use_special_template:\n",
    "    prompt = f\"{instruction_prompt_template}{sample['instruction']}{sample['input']}\\n{response_template}\"\n",
    "else:\n",
    "    chat_temp = [{\"role\": \"system\", \"content\": sample['instruction']},\n",
    "                 {\"role\": \"user\", \"content\": {sample['input']}}]\n",
    "    prompt = tokenizer.apply_chat_template(chat_temp, tokenize=False, add_generation_prompt=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70682a07fcaaca3f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gen_kwargs = {\"max_new_tokens\": 1024}\n",
    "generated_texts = generate(model=model, prompt=prompt, kwargs=gen_kwargs)\n",
    "print(generated_texts)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "febeb00f0a6f0b5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merge to base model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c18abf489437a546"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "clear_hardwares()\n",
    "merged_model = model.merge_and_unload()\n",
    "clear_hardwares()\n",
    "del model\n",
    "new_model_name = 'your_hf_account/your_desired_name'\n",
    "merged_model.push_to_hub(new_model_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f5f450001bf428f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fast Inference with [Vllm](https://github.com/vllm-project/vllm)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4851ef41e4cc4f95"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `vllm` library is one of the fastest inference engines for LLMs. For a comparative overview of available options, you can use this blog: [7 Frameworks for Serving LLMs](https://medium.com/@gsuresh957/7-frameworks-for-serving-llms-5044b533ee88). Importance level: 3\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe82f0a57fe86f60"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "gen_kwargs = {\"max_tokens\": 1024}\n",
    "\n",
    "llm = LLM(model=new_model_name, gpu_memory_utilization=0.9, trust_remote_code=True)\n",
    "sampling_params = SamplingParams(**gen_kwargs)\n",
    "outputs = llm.generate(prompt, gen_kwargs)\n",
    "print(outputs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88bee8960b176e87"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
